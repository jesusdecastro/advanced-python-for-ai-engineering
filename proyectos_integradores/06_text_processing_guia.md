# Gu√≠a del Proyecto: Text Processing Toolkit

## Visi√≥n General

Construir√°s una librer√≠a para procesamiento de texto con an√°lisis b√°sico, transformaciones y extracci√≥n de informaci√≥n. Perfecto para aplicar todos los principios sin dependencias complejas (solo stdlib).

---

## Objetivos de Aprendizaje

Al finalizar este proyecto, habr√°s aplicado:
- Procesamiento de texto con regex
- An√°lisis estad√≠stico de texto
- Generadores para archivos grandes
- Dise√±o extensible con ABCs
- Testing exhaustivo
- Optimizaci√≥n con pandas (opcional)

---

## Dise√±ando la Estructura del Proyecto

### ü§î Preguntas Clave para Dise√±ar tu Estructura

Antes de crear carpetas, piensa en estas preguntas:

**1. ¬øQu√© hace mi toolkit de procesamiento de texto?**
- Lee archivos de texto (TXT, Markdown, etc.)
- Analiza el texto (frecuencias, estad√≠sticas, sentimiento b√°sico)
- Transforma el texto (normalizaci√≥n, limpieza, tokenizaci√≥n)
- Extrae informaci√≥n (emails, URLs, fechas, entidades)

**2. ¬øCu√°les son las operaciones principales?**
- **Leer**: Cargar texto desde archivos
- **Analizar**: Calcular m√©tricas (palabras m√°s frecuentes, longitud promedio)
- **Transformar**: Modificar el texto (lowercase, quitar puntuaci√≥n, tokenizar)
- **Extraer**: Encontrar patrones espec√≠ficos (emails, URLs)
- Cada operaci√≥n es una responsabilidad diferente

**3. ¬øC√≥mo represento un documento?**
- ¬øSolo el texto crudo?
- ¬øTexto + metadata (autor, fecha, fuente)?
- ¬øTexto + an√°lisis pre-calculado?
- Hint: Un modelo de datos (clase o Pydantic) es √∫til

### üí° Pistas de Organizaci√≥n

**Sobre lectura:**
- Diferentes formatos de texto: TXT plano, Markdown, HTML
- Todos devuelven texto, pero el parsing es diferente
- Hint: Clase base abstracta con m√©todo `read()`
- ¬øC√≥mo manejas diferentes encodings (UTF-8, Latin-1)?

**Sobre an√°lisis:**
- Contar palabras es diferente a calcular frecuencias
- Calcular estad√≠sticas (promedio de palabras por oraci√≥n) es otra cosa
- An√°lisis de sentimiento b√°sico (positivo/negativo) es otra
- ¬øCada an√°lisis es una clase o una funci√≥n?
- Hint: Si el an√°lisis tiene estado o configuraci√≥n ‚Üí clase

**Sobre transformaciones:**
- Normalizar: lowercase, quitar acentos, trim
- Limpiar: quitar puntuaci√≥n, n√∫meros, stopwords
- Tokenizar: dividir en palabras, oraciones, n-gramas
- Todas transforman texto ‚Üí texto (o texto ‚Üí lista)
- ¬øNecesitas una interfaz com√∫n?

**Sobre extracci√≥n:**
- Emails: regex `\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`
- URLs: regex para http/https
- Fechas: m√∫ltiples formatos posibles
- Cada extractor busca un patr√≥n espec√≠fico
- Hint: Todos devuelven lista de matches

**Sobre eficiencia:**
- Archivos de texto pueden ser enormes (logs, libros, datasets)
- ¬øCargas todo en memoria o usas generadores?
- Hint: `yield` l√≠nea por l√≠nea o token por token

**Sobre modelos de datos:**
- Un documento tiene: texto, metadata, an√°lisis
- ¬øC√≥mo estructuras esto?
- Hint: Pydantic o dataclass

### üéØ Checklist de Estructura

Antes de empezar a codear, aseg√∫rate de tener:
- [ ] Carpeta `src/` con tu paquete principal
- [ ] M√≥dulo/paquete para readers (lectura de archivos)
- [ ] M√≥dulo/paquete para analyzers (an√°lisis estad√≠stico)
- [ ] M√≥dulo/paquete para transformers (modificaci√≥n de texto)
- [ ] M√≥dulo/paquete para extractors (extracci√≥n de patrones)
- [ ] M√≥dulo/paquete para modelos de datos (Document, AnalysisResult)
- [ ] Carpeta `tests/` con fixtures de textos de ejemplo
- [ ] Carpeta `examples/` con scripts de uso
- [ ] `pyproject.toml` (solo stdlib, pandas opcional)
- [ ] `README.md`

### üöÄ Enfoque Recomendado

1. **Empieza con lectura**: Implementa un reader b√°sico para TXT
2. **A√±ade an√°lisis simple**: Cuenta palabras, calcula frecuencias
3. **A√±ade transformaciones**: Normalizaci√≥n, tokenizaci√≥n
4. **A√±ade extracci√≥n**: Implementa extractor de emails con regex
5. **Generaliza**: Crea abstracciones cuando tengas 2+ implementaciones
6. **Optimiza**: Usa generadores para archivos grandes

**Pregunta gu√≠a**: "Si necesito a√±adir extracci√≥n de n√∫meros de tel√©fono, ¬ød√≥nde va ese c√≥digo?"

---

## Roadmap D√≠a a D√≠a

### D√≠a 1: Fundamentos
**Objetivo:** Lectura y estructura b√°sica

**Tareas:**
- Crear estructura de carpetas con src layout
- Configurar pyproject.toml (solo stdlib, pandas opcional)
- Implementar readers b√°sicos para TXT y Markdown
- Lectura de archivos con diferentes encodings

**Checkpoint:** Puedes leer archivos de texto correctamente

---

### D√≠a 2: C√≥digo Pyth√≥nico
**Objetivo:** Procesamiento eficiente con generadores

**Tareas:**
- Generadores para leer archivos grandes l√≠nea por l√≠nea
- Generadores para tokenizaci√≥n (yield token por token)
- Comprehensions para filtrar palabras
- Decorador @cache_analysis para cachear resultados costosos

**Tips:**
- Para archivos grandes, nunca cargues todo en memoria
- El tokenizador debe hacer yield de cada token
- Comprehensions son perfectas para filtrado de stopwords
- El decorador debe cachear an√°lisis costosos (frecuencias, estad√≠sticas)

**Checkpoint:** Procesas archivos grandes sin problemas de memoria

---

### D√≠a 3: C√≥digo Limpio
**Objetivo:** An√°lisis y transformaci√≥n robusta

**Tareas:**
- Funciones peque√±as: analyze_text ‚Üí count_words, calculate_frequencies, extract_stats
- Excepciones custom: InvalidTextError, ExtractionError, EncodingError
- Docstrings completos con ejemplos
- Validaci√≥n de inputs

**Tips:**
- Cada funci√≥n de an√°lisis debe hacer UNA cosa
- Las excepciones deben ser descriptivas
- Documenta qu√© regex usa cada extractor

**Checkpoint:** El c√≥digo maneja textos problem√°ticos sin crashear

---

### D√≠a 4: Dise√±o
**Objetivo:** Sistema extensible de analyzers y extractors

**Tareas:**
- ABC BaseAnalyzer con m√©todo abstracto analyze()
- Implementar WordCounter, FrequencyAnalyzer, StatisticsAnalyzer
- ABC BaseExtractor con m√©todo abstracto extract()
- Implementar EmailExtractor, URLExtractor, DateExtractor
- Modelos Pydantic: TextDocument(content, metadata, statistics)

**Tips:**
- Cada analyzer produce un tipo espec√≠fico de resultado
- Los extractors usan regex para encontrar patrones
- Pydantic valida la estructura del documento
- Composici√≥n: TextProcessor compone m√∫ltiples Analyzers y Extractors

**Checkpoint:** Puedes a√±adir un nuevo analyzer sin modificar c√≥digo existente

---

### D√≠a 5: Testing y Optimizaci√≥n
**Objetivo:** Tests exhaustivos y optimizaci√≥n

**Tareas:**
- Crear fixtures con textos de ejemplo (diferentes idiomas, formatos)
- Tests para cada analyzer y extractor
- Tests de regex para extracci√≥n
- Optimizar conteos con pandas para textos grandes (opcional)

**Tips:**
- Los fixtures deben incluir casos edge: textos vac√≠os, solo puntuaci√≥n, unicode
- Verifica que las regex funcionan correctamente
- Tests de frecuencias: verifica que los conteos son correctos
- pandas puede acelerar an√°lisis de frecuencias

**Checkpoint:** 80%+ cobertura y an√°lisis correctos

---

### D√≠a 6: Integraci√≥n
**Objetivo:** CLI y reportes de an√°lisis

**Tareas:**
- CLI: textkit analyze --input document.txt --output report.json
- CLI: textkit extract --input document.txt --type emails
- CLI: textkit transform --input document.txt --operation normalize
- README con ejemplos

**Tips:**
- El CLI debe soportar an√°lisis, extracci√≥n y transformaci√≥n
- Los reportes deben ser √∫tiles: estad√≠sticas, frecuencias, extracciones
- Incluye ejemplos de textos en examples/

**Checkpoint:** Puedes analizar un texto y obtener insights √∫tiles

---

## Funcionalidades M√≠nimas Requeridas

### Lectura
- [ ] Leer archivos TXT
- [ ] Leer archivos Markdown
- [ ] Detectar encoding autom√°ticamente
- [ ] Manejo de archivos grandes (generadores)

### An√°lisis
- [ ] Conteo de palabras
- [ ] Conteo de caracteres
- [ ] An√°lisis de frecuencias (palabras m√°s comunes)
- [ ] Estad√≠sticas (promedio palabras por oraci√≥n, etc.)
- [ ] Detecci√≥n de idioma (b√°sico)

### Transformaciones
- [ ] Normalizaci√≥n (lowercase, trim)
- [ ] Limpieza (eliminar puntuaci√≥n, n√∫meros)
- [ ] Tokenizaci√≥n (palabras, oraciones)
- [ ] Eliminaci√≥n de stopwords

### Extracci√≥n
- [ ] Extraer emails con regex
- [ ] Extraer URLs con regex
- [ ] Extraer fechas con regex
- [ ] Extraer n√∫meros de tel√©fono (opcional)

### Reportes
- [ ] Estad√≠sticas generales
- [ ] Top N palabras m√°s frecuentes
- [ ] Informaci√≥n extra√≠da
- [ ] Exportar a JSON/CSV

---

## Criterios de Evaluaci√≥n

1. **Estructura y Organizaci√≥n** (20%)
   - Src layout correcto
   - M√≥dulos bien separados
   - Configuraci√≥n de herramientas

2. **Calidad del C√≥digo** (30%)
   - Regex bien documentadas
   - Funciones peque√±as
   - Docstrings completos
   - Manejo de errores

3. **Dise√±o OOP** (20%)
   - ABCs para analyzers y extractors
   - Modelos Pydantic
   - Composici√≥n
   - Extensibilidad

4. **Testing** (20%)
   - Fixtures con textos variados
   - Tests de an√°lisis
   - Tests de regex
   - 80%+ cobertura

5. **Documentaci√≥n y Usabilidad** (10%)
   - README completo
   - CLI intuitivo
   - Ejemplos claros

---

## Errores Comunes a Evitar

 **Cargar archivos completos en memoria**
 Usa generadores para streaming

 **Regex complejas sin documentar**
 Documenta qu√© busca cada regex

 **No manejar unicode**
 Soporta textos en diferentes idiomas

 **An√°lisis que no maneja casos edge**
 Maneja textos vac√≠os, solo puntuaci√≥n, etc.

 **Extracci√≥n con regex incorrectas**
 Testa las regex exhaustivamente

 **No normalizar antes de analizar**
 Normaliza (lowercase, trim) antes de contar

---

## Recursos √ötiles

- Documentaci√≥n de re (regex en Python)
- Documentaci√≥n de collections (Counter, defaultdict)
- Patrones de regex para emails, URLs, fechas
- Listas de stopwords en diferentes idiomas
- Documentaci√≥n de unicodedata

---

## Preguntas Frecuentes

**P: ¬øDebo usar librer√≠as de NLP como NLTK o spaCy?**
R: No. El objetivo es usar solo stdlib (y pandas opcional). Implementa lo b√°sico t√∫ mismo.

**P: ¬øC√≥mo manejo diferentes idiomas?**
R: Enf√≥cate en ingl√©s y espa√±ol. Usa listas de stopwords y normalizaci√≥n b√°sica.

**P: ¬øQu√© tan complejas deben ser las regex?**
R: B√°sicas pero funcionales. No necesitas regex perfectas, pero deben funcionar en casos comunes.

**P: ¬øDebo implementar stemming o lemmatization?**
R: No es necesario. Enf√≥cate en an√°lisis b√°sico y extracci√≥n.

**P: ¬øC√≥mo optimizo el an√°lisis de frecuencias?**
R: Usa collections.Counter. Para textos muy grandes, considera pandas.

---

## Checklist Final

- [ ] Estructura src layout
- [ ] pyproject.toml configurado
- [ ] Readers con generadores
- [ ] ABCs para analyzers y extractors
- [ ] Modelos Pydantic
- [ ] An√°lisis de frecuencias
- [ ] Extracci√≥n con regex
- [ ] Transformaciones b√°sicas
- [ ] Tests con fixtures
- [ ] 80%+ cobertura
- [ ] CLI funcional
- [ ] README con ejemplos
- [ ] C√≥digo pasa ruff

---

¬°Adelante! El procesamiento de texto es fundamental para NLP y an√°lisis de datos.
