{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Día 5: Memory Profiling y Gestión de Memoria\n",
        "\n",
        "## Descripción General\n",
        "\n",
        "El profiling de memoria es una técnica esencial para identificar y resolver problemas de consumo excesivo de memoria en aplicaciones Python. A medida que trabajas con datasets más grandes o aplicaciones más complejas, entender cómo tu código usa la memoria se vuelve crítico para el rendimiento y la escalabilidad.\n",
        "\n",
        "En este notebook aprenderás a usar memory_profiler para analizar el uso de memoria línea por línea, elegir dtypes apropiados para minimizar el consumo de memoria, y aplicar estrategias de optimización de memoria en Python y pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivos de Aprendizaje\n",
        "\n",
        "Al finalizar este notebook, serás capaz de:\n",
        "\n",
        "1. Usar memory_profiler para analizar el uso de memoria de funciones Python\n",
        "2. Identificar líneas de código que consumen más memoria\n",
        "3. Elegir dtypes apropiados en NumPy y pandas para minimizar memoria\n",
        "4. Aplicar estrategias de optimización de memoria en código Python\n",
        "5. Medir y comparar el impacto de optimizaciones de memoria"
      ]
    }
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introducción a memory_profiler\n",
        "\n",
        "### El Problema que Resuelve\n",
        "\n",
        "Cuando tu aplicación Python consume demasiada memoria, es difícil saber qué parte del código es responsable. Sin herramientas de profiling, solo puedes adivinar dónde está el problema.\n",
        "\n",
        "### La Solución: memory_profiler\n",
        "\n",
        "memory_profiler es una herramienta que mide el uso de memoria línea por línea, mostrando exactamente cuánta memoria consume cada línea de código.\n",
        "\n",
        "### Aprendizaje Clave\n",
        "\n",
        "memory_profiler proporciona análisis línea por línea del uso de memoria, permitiendo identificar exactamente qué operaciones consumen más memoria en tu código.\n",
        "\n",
        "**Referencia oficial:** [memory_profiler PyPI](https://pypi.org/project/memory-profiler/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalación\n",
        "\n",
        "```bash\n",
        "pip install memory-profiler\n",
        "```\n",
        "\n",
        "### Uso Básico\n",
        "\n",
        "Para usar memory_profiler, decora la función que quieres analizar con `@profile`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: This code demonstrates memory_profiler usage\n",
        "# To actually run it, you need to save it to a file and run with:\n",
        "# python -m memory_profiler script.py\n",
        "\n",
        "from memory_profiler import profile\n",
        "\n",
        "@profile\n",
        "def create_large_list():\n",
        "    \"\"\"\n",
        "    Create a large list to demonstrate memory usage.\n",
        "    \n",
        "    :return: Large list of integers\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    # This line allocates memory for 1 million integers\n",
        "    data = list(range(1_000_000))\n",
        "    \n",
        "    # This line creates another list (doubles memory)\n",
        "    doubled = [x * 2 for x in data]\n",
        "    \n",
        "    return doubled\n",
        "\n",
        "# When run with memory_profiler, output shows:\n",
        "# Line #    Mem usage    Increment   Line Contents\n",
        "# ================================================\n",
        "#      3     50.0 MiB     50.0 MiB   @profile\n",
        "#      4                             def create_large_list():\n",
        "#     15     88.5 MiB     38.5 MiB       data = list(range(1_000_000))\n",
        "#     18    127.0 MiB     38.5 MiB       doubled = [x * 2 for x in data]\n",
        "#     20    127.0 MiB      0.0 MiB       return doubled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pregunta de Comprensión\n",
        "\n",
        "¿Por qué es útil ver el uso de memoria línea por línea en lugar de solo el total?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dtypes y Uso de Memoria\n",
        "\n",
        "### El Problema que Resuelve\n",
        "\n",
        "Python y NumPy usan tipos de datos por defecto que a menudo son más grandes de lo necesario. Por ejemplo:\n",
        "- Python integers no tienen límite de tamaño (usan memoria variable)\n",
        "- NumPy usa int64 y float64 por defecto (8 bytes cada uno)\n",
        "- pandas usa object dtype para strings (muy ineficiente)\n",
        "\n",
        "### La Solución: Elegir Dtypes Apropiados\n",
        "\n",
        "Elegir el dtype más pequeño que pueda contener tus datos reduce dramáticamente el uso de memoria.\n",
        "\n",
        "### Aprendizaje Clave\n",
        "\n",
        "Los dtypes determinan cuánta memoria usa cada valor. Elegir el dtype más pequeño apropiado (int8 vs int64, float32 vs float64) puede reducir el uso de memoria en 2x-8x.\n",
        "\n",
        "**Referencia oficial:** [NumPy data types](https://numpy.org/doc/stable/user/basics.types.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Compare memory usage of different dtypes\n",
        "n = 1_000_000\n",
        "\n",
        "# BAD: Using default int64\n",
        "arr_int64 = np.arange(n, dtype='int64')\n",
        "print(f\"int64 array: {arr_int64.nbytes / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# GOOD: Using int32 (if values fit)\n",
        "arr_int32 = np.arange(n, dtype='int32')\n",
        "print(f\"int32 array: {arr_int32.nbytes / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# BETTER: Using int16 (if values fit)\n",
        "arr_int16 = np.arange(min(n, 32767), dtype='int16')\n",
        "print(f\"int16 array: {arr_int16.nbytes / 1024:.2f} KB\")\n",
        "\n",
        "# BEST: Using int8 (if values fit)\n",
        "arr_int8 = np.arange(min(n, 127), dtype='int8')\n",
        "print(f\"int8 array: {arr_int8.nbytes} bytes\")\n",
        "\n",
        "print(f\"\\nMemory reduction int64 -> int32: {(1 - arr_int32.nbytes / arr_int64.nbytes) * 100:.1f}%\")\n",
        "print(f\"Memory reduction int64 -> int8: {(1 - arr_int8.nbytes / (n * 8)) * 100:.1f}%\")"
      ]
    }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tabla de Dtypes y Rangos\n",
        "\n",
        "| Dtype | Bytes | Rango | Uso Recomendado |\n",
        "|-------|-------|-------|----------------|\n",
        "| int8 | 1 | -128 a 127 | Edades, pequeños contadores |\n",
        "| int16 | 2 | -32,768 a 32,767 | Años, IDs pequeños |\n",
        "| int32 | 4 | -2.1B a 2.1B | IDs medianos, contadores |\n",
        "| int64 | 8 | Muy grande | IDs grandes, timestamps |\n",
        "| float32 | 4 | ±3.4e38 | Coordenadas, precios |\n",
        "| float64 | 8 | ±1.8e308 | Cálculos científicos |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estrategias de Optimización de Memoria\n",
        "\n",
        "### Estrategia 1: Usar Generadores en Lugar de Listas\n",
        "\n",
        "Los generadores producen valores uno a la vez, sin almacenar toda la secuencia en memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# BAD: List comprehension loads everything into memory\n",
        "def process_with_list(n: int) -> int:\n",
        "    \"\"\"\n",
        "    Process numbers using list comprehension.\n",
        "    \n",
        "    :param n: Number of items to process\n",
        "    :type n: int\n",
        "    :return: Sum of squares\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    squares = [x**2 for x in range(n)]  # Stores all values\n",
        "    return sum(squares)\n",
        "\n",
        "# GOOD: Generator expression produces values on demand\n",
        "def process_with_generator(n: int) -> int:\n",
        "    \"\"\"\n",
        "    Process numbers using generator expression.\n",
        "    \n",
        "    :param n: Number of items to process\n",
        "    :type n: int\n",
        "    :return: Sum of squares\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    squares = (x**2 for x in range(n))  # Generates values on demand\n",
        "    return sum(squares)\n",
        "\n",
        "# Compare memory usage\n",
        "n = 1_000_000\n",
        "list_result = process_with_list(n)\n",
        "gen_result = process_with_generator(n)\n",
        "\n",
        "print(f\"List result: {list_result}\")\n",
        "print(f\"Generator result: {gen_result}\")\n",
        "print(\"\\nGenerator uses constant memory regardless of n!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estrategia 2: Liberar Memoria Explícitamente\n",
        "\n",
        "Python tiene garbage collection automático, pero puedes ayudar liberando memoria explícitamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def process_large_data():\n",
        "    \"\"\"\n",
        "    Process large data and free memory explicitly.\n",
        "    \"\"\"\n",
        "    # Create large data structure\n",
        "    large_data = list(range(10_000_000))\n",
        "    \n",
        "    # Process it\n",
        "    result = sum(large_data)\n",
        "    \n",
        "    # Free memory explicitly\n",
        "    del large_data\n",
        "    gc.collect()  # Force garbage collection\n",
        "    \n",
        "    return result\n",
        "\n",
        "result = process_large_data()\n",
        "print(f\"Result: {result}\")\n",
        "print(\"Memory freed after processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estrategia 3: Procesar Datos en Chunks\n",
        "\n",
        "En lugar de cargar todo el dataset en memoria, procésalo en pedazos pequeños."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# BAD: Load entire file into memory\n",
        "def process_file_all_at_once(filename: str) -> float:\n",
        "    \"\"\"\n",
        "    Load entire CSV file into memory.\n",
        "    \n",
        "    :param filename: Path to CSV file\n",
        "    :type filename: str\n",
        "    :return: Mean of column\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filename)  # Loads everything\n",
        "    return df['value'].mean()\n",
        "\n",
        "# GOOD: Process file in chunks\n",
        "def process_file_in_chunks(filename: str, chunksize: int = 10000) -> float:\n",
        "    \"\"\"\n",
        "    Process CSV file in chunks to save memory.\n",
        "    \n",
        "    :param filename: Path to CSV file\n",
        "    :type filename: str\n",
        "    :param chunksize: Number of rows per chunk\n",
        "    :type chunksize: int\n",
        "    :return: Mean of column\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    total_sum = 0\n",
        "    total_count = 0\n",
        "    \n",
        "    # Process one chunk at a time\n",
        "    for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
        "        total_sum += chunk['value'].sum()\n",
        "        total_count += len(chunk)\n",
        "    \n",
        "    return total_sum / total_count\n",
        "\n",
        "# Chunked processing uses constant memory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aprendizaje Clave\n",
        "\n",
        "Tres estrategias clave para optimizar memoria: usar generadores en lugar de listas, liberar memoria explícitamente con del y gc.collect(), y procesar datos grandes en chunks.\n",
        "\n",
        "**Referencia oficial:** [Python garbage collection](https://docs.python.org/3/library/gc.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pregunta de Comprensión\n",
        "\n",
        "¿Por qué los generadores usan menos memoria que las listas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Midiendo Uso de Memoria\n",
        "\n",
        "### Usando sys.getsizeof()\n",
        "\n",
        "Python proporciona `sys.getsizeof()` para medir el tamaño de objetos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Compare sizes of different data structures\n",
        "small_list = [1, 2, 3]\n",
        "large_list = list(range(1000))\n",
        "small_tuple = (1, 2, 3)\n",
        "small_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "\n",
        "print(f\"Small list (3 items): {sys.getsizeof(small_list)} bytes\")\n",
        "print(f\"Large list (1000 items): {sys.getsizeof(large_list)} bytes\")\n",
        "print(f\"Small tuple (3 items): {sys.getsizeof(small_tuple)} bytes\")\n",
        "print(f\"Small dict (3 items): {sys.getsizeof(small_dict)} bytes\")\n",
        "\n",
        "# Note: getsizeof() doesn't include referenced objects\n",
        "nested_list = [[1, 2, 3], [4, 5, 6]]\n",
        "print(f\"\\nNested list: {sys.getsizeof(nested_list)} bytes (shallow)\")\n",
        "print(\"Note: This doesn't include the size of inner lists!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usando tracemalloc\n",
        "\n",
        "tracemalloc es un módulo de Python que rastrea asignaciones de memoria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tracemalloc\n",
        "\n",
        "# Start tracing\n",
        "tracemalloc.start()\n",
        "\n",
        "# Take snapshot before\n",
        "snapshot1 = tracemalloc.take_snapshot()\n",
        "\n",
        "# Allocate memory\n",
        "large_list = [i for i in range(1_000_000)]\n",
        "\n",
        "# Take snapshot after\n",
        "snapshot2 = tracemalloc.take_snapshot()\n",
        "\n",
        "# Compare snapshots\n",
        "top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
        "\n",
        "print(\"Top memory allocations:\")\n",
        "for stat in top_stats[:3]:\n",
        "    print(stat)\n",
        "\n",
        "# Stop tracing\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicios Prácticos\n",
        "\n",
        "### Ejercicio 1: Optimizar Dtypes\n",
        "\n",
        "Optimiza el siguiente código para usar menos memoria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Optimize this code\n",
        "import numpy as np\n",
        "\n",
        "# BAD: Using default dtypes\n",
        "ages = np.array([25, 30, 35, 40, 45, 50])  # Values 0-100\n",
        "scores = np.array([85.5, 90.2, 78.8, 92.1, 88.5, 95.0])  # Precision not critical\n",
        "\n",
        "print(f\"Ages memory: {ages.nbytes} bytes\")\n",
        "print(f\"Scores memory: {scores.nbytes} bytes\")\n",
        "\n",
        "# TODO: Create optimized versions with appropriate dtypes\n",
        "# ages_optimized = ...\n",
        "# scores_optimized = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 2: Usar Generadores\n",
        "\n",
        "Reescribe el siguiente código usando generadores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Convert to generator\n",
        "def get_squares(n: int) -> list:\n",
        "    \"\"\"Return list of squares.\"\"\"\n",
        "    return [x**2 for x in range(n)]\n",
        "\n",
        "# TODO: Rewrite as generator function\n",
        "# def get_squares_generator(n: int):\n",
        "#     ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen\n",
        "\n",
        "En este notebook has aprendido:\n",
        "\n",
        "- memory_profiler analiza uso de memoria línea por línea\n",
        "- Los dtypes determinan cuánta memoria usa cada valor\n",
        "- Elegir dtypes apropiados reduce memoria en 2x-8x\n",
        "- Los generadores usan memoria constante vs listas\n",
        "- Procesar datos en chunks mantiene memoria bajo control\n",
        "\n",
        "La optimización de memoria es esencial para trabajar con grandes volúmenes de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preguntas de Autoevaluación\n",
        "\n",
        "### 1. ¿Qué hace memory_profiler?\n",
        "\n",
        "**Respuesta:** memory_profiler mide el uso de memoria línea por línea en funciones Python, mostrando cuánta memoria consume cada línea de código. Esto permite identificar exactamente dónde está el problema de memoria.\n",
        "\n",
        "### 2. ¿Cuál es la diferencia entre int8 e int64?\n",
        "\n",
        "**Respuesta:** int8 usa 1 byte y puede almacenar valores de -128 a 127. int64 usa 8 bytes y puede almacenar valores mucho más grandes. Usar int8 cuando los valores son pequeños reduce memoria en 8x.\n",
        "\n",
        "### 3. ¿Por qué usar generadores en lugar de listas?\n",
        "\n",
        "**Respuesta:** Los generadores producen valores uno a la vez sin almacenar toda la secuencia en memoria. Las listas almacenan todos los valores, consumiendo memoria proporcional al tamaño.\n",
        "\n",
        "### 4. ¿Qué hace gc.collect()?\n",
        "\n",
        "**Respuesta:** gc.collect() fuerza la ejecución del garbage collector de Python, liberando memoria de objetos que ya no están en uso. Útil después de eliminar estructuras grandes con del.\n",
        "\n",
        "### 5. ¿Cuándo procesar datos en chunks?\n",
        "\n",
        "**Respuesta:** Procesa datos en chunks cuando el dataset completo no cabe en memoria. Esto permite trabajar con archivos muy grandes usando memoria constante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recursos y Referencias Oficiales\n",
        "\n",
        "### Documentación Oficial\n",
        "\n",
        "- **memory_profiler**: [https://pypi.org/project/memory-profiler/](https://pypi.org/project/memory-profiler/)\n",
        "  - Herramienta para profiling de memoria línea por línea\n",
        "\n",
        "- **NumPy data types**: [https://numpy.org/doc/stable/user/basics.types.html](https://numpy.org/doc/stable/user/basics.types.html)\n",
        "  - Documentación sobre tipos de datos en NumPy\n",
        "\n",
        "- **Python garbage collection**: [https://docs.python.org/3/library/gc.html](https://docs.python.org/3/library/gc.html)\n",
        "  - Documentación del módulo gc\n",
        "\n",
        "- **Python tracemalloc**: [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html)\n",
        "  - Rastreo de asignaciones de memoria\n",
        "\n",
        "### Guías y Tutoriales\n",
        "\n",
        "- **Memory profiling in Python**: [https://www.datacamp.com/tutorial/memory-profiling-python](https://www.datacamp.com/tutorial/memory-profiling-python)\n",
        "  - Tutorial completo sobre memory profiling\n",
        "\n",
        "- **Python memory management**: [https://realpython.com/python-memory-management/](https://realpython.com/python-memory-management/)\n",
        "  - Guía sobre gestión de memoria en Python\n",
        "\n",
        "### Notas Importantes\n",
        "\n",
        "- Todos los enlaces están actualizados a partir de 2026\n",
        "- memory_profiler requiere instalación: pip install memory-profiler\n",
        "- Elegir dtypes apropiados puede reducir memoria en 2x-8x\n",
        "- Los generadores usan memoria constante independiente del tamaño\n",
        "- Procesar en chunks permite trabajar con datasets que no caben en memoria"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
